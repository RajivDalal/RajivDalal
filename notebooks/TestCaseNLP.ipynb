{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajivDalal/RajivDalal/blob/main/notebooks/TestCaseNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy gensim\n",
        "!pip install tabulate"
      ],
      "metadata": {
        "id": "q0muCI2tmj81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Nidhig19/NLP.git"
      ],
      "metadata": {
        "id": "95Fe6JbTWVZL",
        "outputId": "5284f306-8f2d-42fc-f9a1-2bc3a1e7193f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 84 (delta 17), reused 61 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (84/84), 101.36 KiB | 12.67 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N0NA2SbHsP9",
        "outputId": "e624a888-0ea2-4abe-abb7-f43480b37d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DvIXQUu_LRqE",
        "outputId": "8cbbd22b-6692-41fb-94bf-e4b755fe1dca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ],
      "source": [
        "import spacy, gensim\n",
        "from tabulate import tabulate\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_md')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdFUfJYtAxWp"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U3_dRgYEAzym"
      },
      "outputs": [],
      "source": [
        "#Import and read file\n",
        "with open('NLP/data/sample.txt') as file:\n",
        "    sample = file.read()\n",
        "text = nlp(sample)\n",
        "sentence_spans = list(text.sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6wL3dmjMVEt2",
        "outputId": "82abc387-f39b-414a-f0a7-8d1ef9bc73be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a UI designer, I want to report to the Agencies about user testing, so that they are aware of their contributions to making Broker a better UX.\n",
            "\n",
            "As a Researcher, I want an app that create proxy Data Packages for well know and reliable data, sources, so that I can load high quality data using Data Package tooling. \n",
            "\n",
            "As a participant, I want to change my estimate as long as the draw has not been completed, so that I can change my mind.\n",
            "\n",
            "As a depositor, I want to have metadata automatically filled from other University systems and remembered from previous deposits, so that I don't have to waste time reentering the same information.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sentence in sentence_spans:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcHZawCWBAoL",
        "outputId": "5ab709b1-7014-49ae-fb95-eda95679c0ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 [UI, designer, want, report, Agencies, user, testing, aware, contributions, making, Broker, better, UX]\n",
            "2 [Researcher, want, app, create, proxy, Data, Packages, know, reliable, data, sources, load, high, quality, data, Data, Package, tooling]\n",
            "3 [participant, want, change, estimate, long, draw, completed, change, mind]\n",
            "4 [depositor, want, metadata, automatically, filled, University, systems, remembered, previous, deposits, waste, time, reentering, information]\n"
          ]
        }
      ],
      "source": [
        "#Removing punctuations, stop words, whitespaces\n",
        "sentence_tokens=[]\n",
        "for i,sentence in enumerate(sentence_spans):\n",
        "    filtered_text=[token for token in sentence if not token.is_punct and not token.is_stop and not token.is_space]\n",
        "    sentence_tokens.append(filtered_text)\n",
        "    print(i+1,filtered_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BspGP4tVEt3"
      },
      "source": [
        "# Input, Action and Condition Words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Doc"
      ],
      "metadata": {
        "id": "Zl5yT2H-mvic"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_dep_tree(span, main_verb_index):\n",
        "  words = [token.text for token in span]\n",
        "  new_doc = Doc(span.vocab, words=words)\n",
        "\n",
        "  for i, token in enumerate(span):\n",
        "    new_doc[i].pos_ = token.pos_\n",
        "    new_doc[i].tag_ = token.tag_\n",
        "    new_doc[i].dep_ = token.dep_\n",
        "    head_index = min(token.head.i - span[0].i, len(new_doc) - 1)\n",
        "    new_doc[i].head = new_doc[head_index]\n",
        "\n",
        "  if main_verb_index is not None:\n",
        "    # Set the main verb as root\n",
        "    new_doc[main_verb_index].dep_ = \"ROOT\"\n",
        "    new_doc[main_verb_index].head = new_doc[main_verb_index]\n",
        "\n",
        "  # Adjust dependencies for other tokens\n",
        "  for token in new_doc:\n",
        "      if token.i != main_verb_index and token.dep_ == \"ROOT\":\n",
        "          token.dep_ = \"dep\"\n",
        "          token.head = new_doc[main_verb_index]\n",
        "\n",
        "  return new_doc\n",
        "res = []\n",
        "for id,tokens in enumerate(sentence_tokens):\n",
        "  main_verb_index = None\n",
        "  results = {}\n",
        "  for i, token in enumerate(tokens):\n",
        "\n",
        "    if token.lemma_ == \"want\":\n",
        "      for j, next_token in enumerate(tokens[i:], start=i):\n",
        "        if (next_token.pos_ == \"VERB\" and\n",
        "          next_token.lemma_ not in [\"want\", \"be\", \"have\"] and\n",
        "          not next_token.dep_ == \"aux\"):\n",
        "          main_verb_index = j\n",
        "          break\n",
        "\n",
        "      if main_verb_index:\n",
        "        break\n",
        "\n",
        "  if main_verb_index is not None:\n",
        "\n",
        "    main_verb_text = tokens[main_verb_index].text\n",
        "    orig_main_verb_index = next(\n",
        "        (i for i, token in enumerate(sentence_spans[id])\n",
        "         if token.text == main_verb_text),\n",
        "        None\n",
        "    )\n",
        "\n",
        "    modified_doc = custom_dep_tree(sentence_spans[id], orig_main_verb_index)\n",
        "    main_verb = modified_doc[orig_main_verb_index]\n",
        "    # print(main_verb)\n",
        "\n",
        "    role = None\n",
        "    for token in tokens:\n",
        "      if token.text.lower() == \"as\" and token.i + 2 < len(tokens):\n",
        "        role = tokens[token.i + 2].text\n",
        "        break\n",
        "\n",
        "  results = {\n",
        "          'main_action': main_verb.text,\n",
        "          'role': role or \"Unknown\",\n",
        "          'original_text': sentence_spans[id],\n",
        "          'filtered_text':tokens,\n",
        "          'main_verb_index': main_verb_index,\n",
        "          'dependencies': [(token.text, token.dep_, token.head.text)\n",
        "                          for token in modified_doc if token.i != main_verb_index]\n",
        "      }\n",
        "  res.append(results)\n",
        "  print(results)"
      ],
      "metadata": {
        "id": "Ju1Y6bQmfovd",
        "outputId": "2c03655a-c2e2-4894-f4e1-e75485d7a5f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'main_action': 'report', 'role': 'Unknown', 'original_text': As a UI designer, I want to report to the Agencies about user testing, so that they are aware of their contributions to making Broker a better UX.\n",
            ", 'filtered_text': [UI, designer, want, report, Agencies, user, testing, aware, contributions, making, Broker, better, UX], 'main_verb_index': 3, 'dependencies': [('As', 'prep', 'want'), ('a', 'det', 'designer'), ('UI', 'compound', 'designer'), (',', 'punct', 'want'), ('I', 'nsubj', 'want'), ('want', 'dep', 'report'), ('to', 'aux', 'report'), ('report', 'ROOT', 'report'), ('to', 'prep', 'report'), ('the', 'det', 'Agencies'), ('Agencies', 'pobj', 'to'), ('about', 'prep', 'report'), ('user', 'compound', 'testing'), ('testing', 'pobj', 'about'), (',', 'punct', 'want'), ('so', 'mark', 'are'), ('that', 'mark', 'are'), ('they', 'nsubj', 'are'), ('are', 'advcl', 'want'), ('aware', 'acomp', 'are'), ('of', 'prep', 'aware'), ('their', 'poss', 'contributions'), ('contributions', 'pobj', 'of'), ('to', 'prep', 'contributions'), ('making', 'pcomp', 'to'), ('Broker', 'nsubj', 'UX'), ('a', 'det', 'UX'), ('better', 'amod', 'UX'), ('UX', 'ccomp', 'making'), ('.', 'punct', 'want'), ('\\n', 'dep', '.')]}\n",
            "{'main_action': 'create', 'role': 'Unknown', 'original_text': As a Researcher, I want an app that create proxy Data Packages for well know and reliable data, sources, so that I can load high quality data using Data Package tooling. \n",
            ", 'filtered_text': [Researcher, want, app, create, proxy, Data, Packages, know, reliable, data, sources, load, high, quality, data, Data, Package, tooling], 'main_verb_index': 3, 'dependencies': [('As', 'prep', 'want'), ('a', 'det', 'Researcher'), ('Researcher', 'pobj', 'As'), ('I', 'nsubj', 'want'), ('want', 'dep', 'create'), ('an', 'det', 'app'), ('app', 'dobj', 'want'), ('that', 'nsubj', 'create'), ('create', 'ROOT', 'create'), ('proxy', 'compound', 'Packages'), ('Data', 'compound', 'Packages'), ('Packages', 'dobj', 'create'), ('for', 'prep', 'create'), ('well', 'advmod', 'know'), ('know', 'pobj', 'for'), ('and', 'cc', 'know'), ('reliable', 'conj', 'know'), ('data', 'pobj', 'for'), (',', 'punct', 'data'), ('sources', 'conj', 'data'), (',', 'punct', 'want'), ('so', 'mark', 'load'), ('that', 'mark', 'load'), ('I', 'nsubj', 'load'), ('can', 'aux', 'load'), ('load', 'advcl', 'want'), ('high', 'amod', 'quality'), ('quality', 'compound', 'data'), ('data', 'dobj', 'load'), ('using', 'acl', 'data'), ('Data', 'compound', 'Package'), ('Package', 'compound', 'tooling'), ('tooling', 'dobj', 'using'), ('.', 'punct', 'want'), ('\\n', 'dep', '.')]}\n",
            "{'main_action': 'change', 'role': 'Unknown', 'original_text': As a participant, I want to change my estimate as long as the draw has not been completed, so that I can change my mind.\n",
            ", 'filtered_text': [participant, want, change, estimate, long, draw, completed, change, mind], 'main_verb_index': 2, 'dependencies': [('As', 'prep', 'want'), ('a', 'det', 'participant'), (',', 'punct', 'want'), ('I', 'nsubj', 'want'), ('want', 'dep', 'change'), ('to', 'aux', 'change'), ('change', 'ROOT', 'change'), ('my', 'poss', 'estimate'), ('estimate', 'dobj', 'change'), ('as', 'advmod', 'long'), ('long', 'advmod', 'change'), ('as', 'mark', 'completed'), ('the', 'det', 'draw'), ('draw', 'nsubjpass', 'completed'), ('has', 'aux', 'completed'), ('not', 'neg', 'completed'), ('been', 'auxpass', 'completed'), ('completed', 'advcl', 'long'), (',', 'punct', 'want'), ('so', 'mark', 'change'), ('that', 'mark', 'change'), ('I', 'nsubj', 'change'), ('can', 'aux', 'change'), ('change', 'advcl', 'want'), ('my', 'poss', 'mind'), ('mind', 'dobj', 'change'), ('.', 'punct', 'want'), ('\\n', 'dep', '.')]}\n",
            "{'main_action': 'filled', 'role': 'Unknown', 'original_text': As a depositor, I want to have metadata automatically filled from other University systems and remembered from previous deposits, so that I don't have to waste time reentering the same information.\n",
            ", 'filtered_text': [depositor, want, metadata, automatically, filled, University, systems, remembered, previous, deposits, waste, time, reentering, information], 'main_verb_index': 4, 'dependencies': [('As', 'prep', 'want'), ('a', 'det', 'depositor'), ('depositor', 'pobj', 'As'), (',', 'punct', 'want'), ('want', 'dep', 'filled'), ('to', 'aux', 'have'), ('have', 'xcomp', 'want'), ('metadata', 'dobj', 'have'), ('automatically', 'advmod', 'filled'), ('filled', 'ROOT', 'filled'), ('from', 'prep', 'filled'), ('other', 'amod', 'systems'), ('University', 'compound', 'systems'), ('systems', 'pobj', 'from'), ('and', 'cc', 'have'), ('remembered', 'conj', 'have'), ('from', 'prep', 'remembered'), ('previous', 'amod', 'deposits'), ('deposits', 'pobj', 'from'), (',', 'punct', 'remembered'), ('so', 'mark', 'have'), ('that', 'mark', 'have'), ('I', 'nsubj', 'have'), ('do', 'aux', 'have'), (\"n't\", 'neg', 'have'), ('have', 'advcl', 'remembered'), ('to', 'aux', 'waste'), ('waste', 'xcomp', 'have'), ('time', 'dobj', 'waste'), ('reentering', 'advcl', 'waste'), ('the', 'det', 'information'), ('same', 'amod', 'information'), ('information', 'dobj', 'reentering'), ('.', 'punct', 'want'), ('\\n', 'dep', '.')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEqYxar6VEt2"
      },
      "source": [
        "# Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TUyMlmZC35l"
      },
      "outputs": [],
      "source": [
        "for i,tokens in enumerate(sentence_tokens):\n",
        "    data = []\n",
        "    #displacy.render(sentence_spans[i], style=\"dep\")\n",
        "    for token in tokens:\n",
        "        data.append([token.text, token.lemma_, token.pos_, token.tag_, token.dep_])\n",
        "\n",
        "    # Print the table\n",
        "    headers = [\"Text\", \"Lemma\", \"POS\", \"Tag\", \"Dependency\"]\n",
        "    print(tabulate(data, headers=headers, tablefmt=\"fancy_grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "def extract_main_actions(user_stories):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    results = {}\n",
        "\n",
        "    for story in user_stories:\n",
        "        doc = nlp(story)\n",
        "\n",
        "        # Find the \"want to\" pattern and get the main verb that follows\n",
        "        main_verb = None\n",
        "        for i, token in enumerate(doc):\n",
        "            # Look for the pattern \"want to [VERB]\" or \"want [VERB]\"\n",
        "            if token.lemma_ == \"want\":\n",
        "                # Look ahead for the main verb\n",
        "                for next_token in doc[i:]:\n",
        "                    if (next_token.pos_ == \"VERB\" and\n",
        "                        next_token.lemma_ not in [\"want\", \"be\", \"have\"] and\n",
        "                        not next_token.dep_ == \"aux\"):\n",
        "                        main_verb = next_token\n",
        "                        break\n",
        "\n",
        "        if main_verb:\n",
        "            # Find the role (usually after \"As a\")\n",
        "            role = None\n",
        "            for token in doc:\n",
        "                if token.text.lower() == \"as\" and token.i + 2 < len(doc):\n",
        "                    role = doc[token.i + 2].text\n",
        "                    break\n",
        "\n",
        "            # Get the surrounding context\n",
        "            context = {\n",
        "                'main_action': main_verb.lemma_,\n",
        "                'original_text': story,\n",
        "                'role': role or \"Unknown\",\n",
        "                'object': [token.text for token in main_verb.children if token.dep_ in [\"dobj\", \"pobj\"]],\n",
        "                'sentence_structure': [(token.text, token.dep_, token.pos_) for token in doc]\n",
        "            }\n",
        "            results[story] = context\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "user_stories = [\n",
        "    \"As a UI designer, I want to report to the Agencies about user testing, so that they are aware of their contributions to making Broker a better UX.\",\n",
        "    \"As a Researcher, I want an app that create proxy Data Packages for well know and reliable data, sources, so that I can load high quality data using Data Package tooling.\",\n",
        "    \"As a participant, I want to change my estimate as long as the draw has not been completed, so that I can change my mind.\",\n",
        "    \"As a depositor, I want to have metadata automatically filled from other University systems and remembered from previous deposits, so that I don't have to waste time reentering the same information.\"\n",
        "]\n",
        "\n",
        "# Extract and print results\n",
        "results = extract_main_actions(user_stories)\n",
        "\n",
        "# Print formatted results\n",
        "for story, info in results.items():\n",
        "    print(\"\\nUser Story Analysis:\")\n",
        "    print(f\"Role: {info['role']}\")\n",
        "    print(f\"Main Action: {info['main_action']}\")\n",
        "    print(f\"Object: {', '.join(info['object']) if info['object'] else 'None'}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "4lrLz32PXRUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RkwgzUeVEt3",
        "outputId": "c29e1872-b8ff-4428-d5b3-2ae1b00af1cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UI, designer, want, report, Agencies, user, testing, aware, contributions, making, Broker, better, UX]\n",
            "{'user_story_id': 1, 'action': 'report', 'inputs': ['UI designer', 'designer'], 'conditions': []} \n",
            "\n",
            "[Researcher, want, app, create, proxy, Data, Packages, know, reliable, data, sources, load, high, quality, data, Data, Package, tooling]\n",
            "{'user_story_id': 2, 'action': '', 'inputs': ['Researcher', 'app', 'proxy Packages', 'Data Packages', 'Packages', 'know', 'data', 'quality data', 'data', 'Package tooling', 'tooling'], 'conditions': ['so that I can load high quality data using Data Package tooling']} \n",
            "\n",
            "[participant, want, change, estimate, long, draw, completed, change, mind]\n",
            "{'user_story_id': 3, 'action': 'change', 'inputs': ['participant'], 'conditions': []} \n",
            "\n",
            "[depositor, want, metadata, automatically, filled, University, systems, remembered, previous, deposits, waste, time, reentering, information]\n",
            "{'user_story_id': 4, 'action': '', 'inputs': ['depositor', 'metadata', 'University systems', 'systems', 'deposits', 'time', 'information'], 'conditions': ['reentering the same information']} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "uid = 1\n",
        "for tokens in sentence_tokens:\n",
        "    print(tokens)\n",
        "    action = \"\"\n",
        "    inputs = []\n",
        "    condns = []\n",
        "    for token in tokens:\n",
        "        if token.dep_ in [\"xcomp\",\"relcl\"] and token.head.dep_ == \"ROOT\":\n",
        "            action = token.text\n",
        "            break\n",
        "\n",
        "        if token.dep_ in [\"dobj\",\"pobj\",\"attr\"]:\n",
        "            inputs.append(token.text)\n",
        "        if token.dep_ == \"compound\" and token.head.dep_ in [\"dobj\",\"pobj\",\"attr\"]:\n",
        "            inputs.append(token.text+\" \"+token.head.text)\n",
        "\n",
        "        if token.dep_ in [\"advcl\",\"ccomp\"]:\n",
        "            condns.append(\" \".join([child.text for child in token.subtree]))\n",
        "\n",
        "    result = {\n",
        "        \"user_story_id\": uid,\n",
        "        \"action\": action,\n",
        "        \"inputs\": inputs,\n",
        "        \"conditions\": condns,\n",
        "    }\n",
        "    print(result,\"\\n\")\n",
        "    uid+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhfEAAD8VEt3"
      },
      "source": [
        "# Test Case Generation using Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcPYBiAtVEt3",
        "outputId": "b764b6f1-11b6-4a4c-e778-87fe688d8714",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate bitsandbytes transformers\n",
        "!pip install torch\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "_NSyWeAeY8jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n"
      ],
      "metadata": {
        "id": "WxuxKCjGbGtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "42rbBizhefSZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}